{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL Assignment 1: Imitation Learning\n",
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import sys,os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_util\n",
    "import gym\n",
    "\n",
    "home_work_root=os.path.abspath('..')\n",
    "\n",
    "\n",
    "def collect_expert_data(env_name, max_timesteps=None, num_rollouts=20, render=False):\n",
    "    from load_policy import load_policy\n",
    "    policy_fn = load_policy(os.path.join(home_work_root, 'experts', env_name+'.pkl'))\n",
    "    \n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "        env = gym.make(env_name)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                obs, r, done, _ = env.step(action)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if steps % 10 == 0:\n",
    "                    print(\"\\rIter{0}/{1}: {2}/{3}\".format(i+1, num_rollouts, steps+1, max_steps), end='')\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "        print('. done')\n",
    "\n",
    "        #print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "    return {'observation': observations, 'actions': actions}\n",
    "\n",
    "def test_policy(env_name, policy, max_timesteps=None, num_rollouts=20, render=False):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-04 22:15:16,186] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter20/20: 1001/1000. done\n",
      "mean return 10412.4788075\n",
      "std of return 62.6522079497\n"
     ]
    }
   ],
   "source": [
    "env_names=['Humanoid-v1',]\n",
    "expert_data={env: collect_expert_data(env, num_rollouts=20) for env in env_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(env_name):\n",
    "    pass\n",
    "\n",
    "for env_name in expert_data:\n",
    "    obs = expert_data[env_name]['observation']\n",
    "    actions = expert_data[env_name]['actions']\n",
    "    y_, x_,  = build_model(env_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
